import torch
import os
import tempfile
import torchaudio
import json
import numpy as np
import random
# ä¿®æ”¹å¯¼å…¥è·¯å¾„ï¼Œä» utils_asr å¯¼å…¥
from .utils_asr import load_asr_model, unload_asr_model

# ASR è¯­è¨€åˆ—è¡¨
ASR_LANGUAGES = {
    "è‡ªåŠ¨è¯†åˆ« (Auto)": None,
    "ä¸­æ–‡ (Chinese)": "Chinese",
    "è‹±è¯­ (English)": "English",
    "ç²¤è¯­ (Cantonese)": "Cantonese",
    "æ—¥è¯­ (Japanese)": "Japanese",
    "éŸ©è¯­ (Korean)": "Korean",
    "æ³•è¯­ (French)": "French",
    "å¾·è¯­ (German)": "German",
    "è¥¿ç­ç‰™è¯­ (Spanish)": "Spanish",
    "ä¿„è¯­ (Russian)": "Russian",
    "æ„å¤§åˆ©è¯­ (Italian)": "Italian",
    "è‘¡è„ç‰™è¯­ (Portuguese)": "Portuguese",
    "æ³°è¯­ (Thai)": "Thai",
    "è¶Šå—è¯­ (Vietnamese)": "Vietnamese",
    "é˜¿æ‹‰ä¼¯è¯­ (Arabic)": "Arabic",
    "å°å°¼è¯­ (Indonesian)": "Indonesian",
    "åœŸè€³å…¶è¯­ (Turkish)": "Turkish",
    "å°åœ°è¯­ (Hindi)": "Hindi",
    "é©¬æ¥è¯­ (Malay)": "Malay",
    "è·å…°è¯­ (Dutch)": "Dutch",
    "ç‘å…¸è¯­ (Swedish)": "Swedish",
    "ä¸¹éº¦è¯­ (Danish)": "Danish",
    "èŠ¬å…°è¯­ (Finnish)": "Finnish",
    "æ³¢å…°è¯­ (Polish)": "Polish",
    "æ·å…‹è¯­ (Czech)": "Czech",
    "è²å¾‹å®¾è¯­ (Filipino)": "Filipino",
    "æ³¢æ–¯è¯­ (Persian)": "Persian",
    "å¸Œè…Šè¯­ (Greek)": "Greek",
    "åŒˆç‰™åˆ©è¯­ (Hungarian)": "Hungarian",
    "é©¬å…¶é¡¿è¯­ (Macedonian)": "Macedonian",
    "ç½—é©¬å°¼äºšè¯­ (Romanian)": "Romanian"
}

class Qwen_ASR_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        presets = ["Qwen3-ASR-1.7B", "Qwen3-ASR-0.6B"]
        return {
            "required": {
                # å…¨ä¸­æ–‡ç»„ä»¶åç§°
                "éŸ³é¢‘": ("AUDIO", ),
                "æ¨¡å‹åç§°": (presets, {"default": presets[0]}),
                "è¯­è¨€": (list(ASR_LANGUAGES.keys()), {"default": "è‡ªåŠ¨è¯†åˆ« (Auto)"}),
                
                # --- æ¨ç†å‚æ•° ---
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 256, "min": 64, "max": 2048, "step": 64}),
                "æ‰¹å¤„ç†å¤§å°": ("INT", {"default": 1, "min": 1, "max": 32}),
                "ç”Ÿæˆæ—¶é—´æˆ³": ("BOOLEAN", {"default": False, "label": "ç”Ÿæˆæ—¶é—´æˆ³ (éœ€ä¸‹è½½é¢å¤–æ¨¡å‹)"}),
                
                # --- ä¸‹è½½è®¾ç½® ---
                "ä¸‹è½½æº": (["ModelScope", "HuggingFace", "HF Mirror"], {"default": "ModelScope"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("æ–‡æœ¬è¾“å‡º", "JSONè¯¦ç»†æ•°æ®")
    FUNCTION = "transcribe_audio"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "ä½¿ç”¨ Qwen3-ASR è¿›è¡Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ã€‚å¼€å¯æ—¶é—´æˆ³å°†è‡ªåŠ¨ä¸‹è½½ Qwen3-ForcedAlignerã€‚"

    def _save_temp_wav(self, audio_input):
        """å°† ComfyUI çš„éŸ³é¢‘ Tensor ä¿å­˜ä¸ºä¸´æ—¶ WAV æ–‡ä»¶"""
        waveform = audio_input['waveform'] 
        sample_rate = audio_input['sample_rate']
        
        if waveform.dim() == 3:
            wav_tensor = waveform[0]
        else:
            wav_tensor = waveform

        if wav_tensor.shape[0] > wav_tensor.shape[1]: 
             wav_tensor = wav_tensor.t()

        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        temp_file.close()
        
        torchaudio.save(temp_file.name, wav_tensor.cpu(), sample_rate)
        return temp_file.name

    def transcribe_audio(self, éŸ³é¢‘, æ¨¡å‹åç§°, è¯­è¨€, æœ€å¤§ç”Ÿæˆé•¿åº¦, æ‰¹å¤„ç†å¤§å°, ç”Ÿæˆæ—¶é—´æˆ³, ä¸‹è½½æº, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹):
        
        temp_wav_path = None
        try:
            # 1. å‡†å¤‡éŸ³é¢‘æ–‡ä»¶
            temp_wav_path = self._save_temp_wav(éŸ³é¢‘)
            
            # 2. åŠ è½½æ¨¡å‹
            model = load_asr_model(
                æ¨¡å‹åç§°, 
                self.device, 
                è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, 
                source=ä¸‹è½½æº, 
                use_aligner=ç”Ÿæˆæ—¶é—´æˆ³
            )

            # 3. å‡†å¤‡å‚æ•°
            target_lang = ASR_LANGUAGES.get(è¯­è¨€, None)
            
            print(f"[Qwen ASR] Transcribing... Lang: {target_lang if target_lang else 'Auto'} | Timestamps: {ç”Ÿæˆæ—¶é—´æˆ³}")
            
            # 4. æ‰§è¡Œæ¨ç†
            results = model.transcribe(
                audio=[temp_wav_path],
                language=[target_lang] if target_lang else None,
                return_time_stamps=ç”Ÿæˆæ—¶é—´æˆ³,
                max_new_tokens=æœ€å¤§ç”Ÿæˆé•¿åº¦,
                batch_size=æ‰¹å¤„ç†å¤§å°
            )

            # 5. å¤„ç†ç»“æœ
            result = results[0]
            text_output = result.text
            
            # æ„å»ºè¯¦ç»† JSON è¾“å‡º
            json_data = {
                "language": result.language,
                "text": result.text,
            }
            if ç”Ÿæˆæ—¶é—´æˆ³ and hasattr(result, 'time_stamps'):
                json_data["timestamps"] = result.time_stamps

            print(f"[Qwen ASR] Detected: {result.language}")
            print(f"[Qwen ASR] Text: {text_output[:50]}...")

            return (text_output, json.dumps(json_data, ensure_ascii=False, indent=2))

        except Exception as e:
            import traceback
            traceback.print_exc()
            raise Exception(f"ASR Error: {str(e)}")
            
        finally:
            if temp_wav_path and os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)
            
            unload_asr_model()