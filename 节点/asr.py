import torch
import os
import tempfile
import torchaudio
import json
import numpy as np
import random
import gc
# å¼•å…¥ ComfyUI çš„å·¥å…·ä»¥æ”¯æŒè¿›åº¦æ¡
import nodes
from comfy.utils import ProgressBar

# ä» utils_asr å¯¼å…¥
from .utils_asr import load_asr_model, unload_asr_model

# ================= é…ç½®ä¸å¸¸é‡ =================

ASR_LANGUAGES = {
    "è‡ªåŠ¨è¯†åˆ« (Auto)": None,
    "ä¸­æ–‡ (Chinese)": "Chinese",
    "è‹±è¯­ (English)": "English",
    "ç²¤è¯­ (Cantonese)": "Cantonese",
    "æ—¥è¯­ (Japanese)": "Japanese",
    "éŸ©è¯­ (Korean)": "Korean",
    "æ³•è¯­ (French)": "French",
    "å¾·è¯­ (German)": "German",
    "è¥¿ç­ç‰™è¯­ (Spanish)": "Spanish",
    "ä¿„è¯­ (Russian)": "Russian",
    "æ„å¤§åˆ©è¯­ (Italian)": "Italian",
    "è‘¡è„ç‰™è¯­ (Portuguese)": "Portuguese",
    "æ³°è¯­ (Thai)": "Thai",
    "è¶Šå—è¯­ (Vietnamese)": "Vietnamese",
    "é˜¿æ‹‰ä¼¯è¯­ (Arabic)": "Arabic",
    "å°å°¼è¯­ (Indonesian)": "Indonesian",
    "åœŸè€³å…¶è¯­ (Turkish)": "Turkish",
    "å°åœ°è¯­ (Hindi)": "Hindi",
    "é©¬æ¥è¯­ (Malay)": "Malay",
    "è·å…°è¯­ (Dutch)": "Dutch",
    "ç‘å…¸è¯­ (Swedish)": "Swedish",
    "ä¸¹éº¦è¯­ (Danish)": "Danish",
    "èŠ¬å…°è¯­ (Finnish)": "Finnish",
    "æ³¢å…°è¯­ (Polish)": "Polish",
    "æ·å…‹è¯­ (Czech)": "Czech",
    "è²å¾‹å®¾è¯­ (Filipino)": "Filipino",
    "æ³¢æ–¯è¯­ (Persian)": "Persian",
    "å¸Œè…Šè¯­ (Greek)": "Greek",
    "åŒˆç‰™åˆ©è¯­ (Hungarian)": "Hungarian",
    "é©¬å…¶é¡¿è¯­ (Macedonian)": "Macedonian",
    "ç½—é©¬å°¼äºšè¯­ (Romanian)": "Romanian"
}

# ================= è¾…åŠ©å‡½æ•° =================

def save_audio_to_temp(audio_input):
    """
    å°† ComfyUI çš„éŸ³é¢‘ Tensor ä¿å­˜ä¸ºä¸´æ—¶ WAV æ–‡ä»¶ã€‚
    """
    try:
        waveform = audio_input['waveform'] 
        sample_rate = audio_input['sample_rate']
        
        # å¤„ç†ç»´åº¦
        if waveform.dim() == 3:
            wav_tensor = waveform[0]
        else:
            wav_tensor = waveform

        if wav_tensor.shape[0] > wav_tensor.shape[1]: 
             wav_tensor = wav_tensor.t()

        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        temp_file.close()
        
        torchaudio.save(temp_file.name, wav_tensor.cpu(), sample_rate)
        return temp_file.name
    except Exception as e:
        print(f"[ASR Error] Failed to save temp audio: {e}")
        return None

def format_timestamps(result):
    """å°†æ—¶é—´æˆ³ç»“æœæ ¼å¼åŒ–ä¸ºæ˜“è¯»å­—ç¬¦ä¸²"""
    if not hasattr(result, 'time_stamps') or not result.time_stamps:
        return ""
    
    lines = []
    for ts in result.time_stamps:
        # å…¼å®¹æ€§å¤„ç†ï¼šè·å–å±æ€§
        start = getattr(ts, 'start_time', 0.0)
        end = getattr(ts, 'end_time', 0.0)
        text = getattr(ts, 'text', '')
        lines.append(f"{start:.2f}-{end:.2f}: {text}")
    return "\n".join(lines)

def result_to_dict_list(time_stamps):
    """[ä¿®å¤] å°† ForcedAlignResult å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºæ™®é€šçš„å­—å…¸åˆ—è¡¨ï¼Œä»¥ä¾¿ JSON åºåˆ—åŒ–"""
    dict_list = []
    if not time_stamps:
        return dict_list
        
    for ts in time_stamps:
        dict_list.append({
            "start": getattr(ts, 'start_time', 0.0),
            "end": getattr(ts, 'end_time', 0.0),
            "text": getattr(ts, 'text', '')
        })
    return dict_list

# ================= èŠ‚ç‚¹ 1: æ ‡å‡† ASR èŠ‚ç‚¹ =================

class Qwenè¯­éŸ³è¯†åˆ«_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        presets = ["Qwen3-ASR-1.7B", "Qwen3-ASR-0.6B"]
        return {
            "required": {
                "éŸ³é¢‘": ("AUDIO", ),
                "æ¨¡å‹åç§°": (presets, {"default": presets[0]}),
                "è¯­è¨€": (list(ASR_LANGUAGES.keys()), {"default": "è‡ªåŠ¨è¯†åˆ« (Auto)"}),
                "æç¤ºè¯": ("STRING", {"multiline": True, "default": "", "placeholder": "å¯é€‰ï¼šè¾“å…¥ä¸Šä¸‹æ–‡æˆ–æç¤ºè¯"}),
                "ç”Ÿæˆæ—¶é—´æˆ³": ("BOOLEAN", {"default": False, "label": "ç”Ÿæˆæ—¶é—´æˆ³ (éœ€ä¸‹è½½é¢å¤–æ¨¡å‹)"}),
                "ä¸‹è½½æº": (["ModelScope", "HuggingFace", "HF Mirror"], {"default": "ModelScope"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("æ–‡æœ¬è¾“å‡º", "JSONè¯¦ç»†æ•°æ®", "å¸¦æ—¶é—´æˆ³æ–‡æœ¬")
    FUNCTION = "transcribe_audio"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½/è¯­éŸ³è¯†åˆ«"
    DESCRIPTION = "ä½¿ç”¨ Qwen3-ASR è¿›è¡Œè¯­éŸ³è¯†åˆ«ã€‚"

    def transcribe_audio(self, éŸ³é¢‘, æ¨¡å‹åç§°, è¯­è¨€, æç¤ºè¯, ç”Ÿæˆæ—¶é—´æˆ³, ä¸‹è½½æº, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹):
        
        temp_wav_path = None
        try:
            # 1. å‡†å¤‡éŸ³é¢‘
            temp_wav_path = save_audio_to_temp(éŸ³é¢‘)
            if not temp_wav_path:
                raise ValueError("Audio processing failed.")

            # 2. åŠ è½½æ¨¡å‹
            model = load_asr_model(
                æ¨¡å‹åç§°, 
                self.device, 
                è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, 
                source=ä¸‹è½½æº, 
                use_aligner=ç”Ÿæˆæ—¶é—´æˆ³
            )

            # 3. å‡†å¤‡å‚æ•°
            target_lang = ASR_LANGUAGES.get(è¯­è¨€, None)
            context_prompt = æç¤ºè¯.strip() if æç¤ºè¯.strip() else None

            print(f"[Qwen ASR] Transcribing... Lang: {target_lang if target_lang else 'Auto'}")
            
            # 4. æ‰§è¡Œæ¨ç†
            kwargs = {
                "audio": [temp_wav_path],
                "language": [target_lang] if target_lang else None,
                "return_time_stamps": ç”Ÿæˆæ—¶é—´æˆ³
            }
            if context_prompt:
                kwargs["context"] = [context_prompt]

            results = model.transcribe(**kwargs)

            # 5. å¤„ç†ç»“æœ
            result = results[0]
            text_output = result.text
            
            # æ„å»ºè¯¦ç»† JSON
            json_data = {
                "language": result.language,
                "text": result.text,
                "timestamps": []
            }
            
            formatted_ts = ""
            if ç”Ÿæˆæ—¶é—´æˆ³:
                if hasattr(result, 'time_stamps') and result.time_stamps:
                    # [ä¿®å¤] å¿…é¡»å…ˆè½¬æˆå­—å…¸ï¼Œå¦åˆ™ json.dumps ä¼šæŠ¥é”™
                    json_data["timestamps"] = result_to_dict_list(result.time_stamps)
                    formatted_ts = format_timestamps(result)

            print(f"[Qwen ASR] Detected: {result.language}")
            print(f"[Qwen ASR] Text: {text_output[:50]}...")

            return (text_output, json.dumps(json_data, ensure_ascii=False, indent=2), formatted_ts)

        except Exception as e:
            import traceback
            traceback.print_exc()
            raise Exception(f"ASR Error: {str(e)}")
            
        finally:
            if temp_wav_path and os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)

# ================= èŠ‚ç‚¹ 2: æ‰¹é‡ ASR èŠ‚ç‚¹ =================

class Qwenæ‰¹é‡è¯­éŸ³è¯†åˆ«_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        presets = ["Qwen3-ASR-1.7B", "Qwen3-ASR-0.6B"]
        return {
            "required": {
                "éŸ³é¢‘åˆ—è¡¨": ("AUDIO", ), 
                "æ¨¡å‹åç§°": (presets, {"default": presets[0]}),
                "è¯­è¨€": (list(ASR_LANGUAGES.keys()), {"default": "è‡ªåŠ¨è¯†åˆ« (Auto)"}),
                "æç¤ºè¯": ("STRING", {"multiline": True, "default": "", "placeholder": "æ‰¹é‡æç¤ºè¯"}),
                "ç”Ÿæˆæ—¶é—´æˆ³": ("BOOLEAN", {"default": False}),
                "ä¸‹è½½æº": (["ModelScope", "HuggingFace", "HF Mirror"], {"default": "ModelScope"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("åˆå¹¶æ–‡æœ¬", "è¯¦ç»†æ—¥å¿—æ–‡æœ¬", "å¸¦æ—¶é—´æˆ³æ–‡æœ¬")
    FUNCTION = "batch_transcribe"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½/è¯­éŸ³è¯†åˆ«"
    DESCRIPTION = "æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘ç‰‡æ®µï¼Œè¾“å‡ºåˆå¹¶åçš„æ–‡æœ¬ã€‚"

    def batch_transcribe(self, éŸ³é¢‘åˆ—è¡¨, æ¨¡å‹åç§°, è¯­è¨€, æç¤ºè¯, ç”Ÿæˆæ—¶é—´æˆ³, ä¸‹è½½æº, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹):
        temp_files = []
        try:
            audio_inputs = []
            if isinstance(éŸ³é¢‘åˆ—è¡¨, list):
                audio_inputs = éŸ³é¢‘åˆ—è¡¨
            else:
                audio_inputs = [éŸ³é¢‘åˆ—è¡¨]

            total_files = len(audio_inputs)
            if total_files == 0:
                return ("", "", "")

            # è¿›åº¦æ¡
            pbar = ProgressBar(total_files)

            model = load_asr_model(
                æ¨¡å‹åç§°, 
                self.device, 
                è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, 
                source=ä¸‹è½½æº, 
                use_aligner=ç”Ÿæˆæ—¶é—´æˆ³
            )
            
            target_lang = ASR_LANGUAGES.get(è¯­è¨€, None)
            context_prompt = æç¤ºè¯.strip() if æç¤ºè¯.strip() else None
            
            full_text_list = []
            log_lines = []
            timestamp_text_list = []

            print(f"[Qwen ASR Batch] Processing {total_files} files...")

            for i, audio_item in enumerate(audio_inputs):
                temp_path = None
                try:
                    temp_path = save_audio_to_temp(audio_item)
                    if not temp_path:
                        continue
                    
                    kwargs = {
                        "audio": [temp_path],
                        "language": [target_lang] if target_lang else None,
                        "return_time_stamps": ç”Ÿæˆæ—¶é—´æˆ³
                    }
                    if context_prompt:
                        kwargs["context"] = [context_prompt]

                    results = model.transcribe(**kwargs)
                    res = results[0]

                    # 1. æ–‡æœ¬
                    full_text_list.append(res.text)
                    
                    # 2. æ—¶é—´æˆ³
                    current_ts_str = ""
                    if ç”Ÿæˆæ—¶é—´æˆ³ and hasattr(res, 'time_stamps'):
                        current_ts_str = format_timestamps(res)
                    
                    filename = audio_item.get("filename", f"Audio_{i+1}")

                    if current_ts_str:
                        timestamp_text_list.append(f"--- {filename} ---")
                        timestamp_text_list.append(current_ts_str)
                        timestamp_text_list.append("")
                    
                    # 3. æ—¥å¿—
                    log_lines.append(f"--- [{i+1}/{total_files}] {filename} ({res.language}) ---")
                    log_lines.append(res.text)
                    if current_ts_str:
                        log_lines.append("[Timestamps]")
                        log_lines.append(current_ts_str)
                    log_lines.append("")

                except Exception as inner_e:
                    print(f"[Error] Batch processing failed at index {i}: {inner_e}")
                    full_text_list.append(f"[Error in file {i+1}]")
                
                finally:
                    if temp_path and os.path.exists(temp_path):
                        os.remove(temp_path)
                    pbar.update(1)

            return ("\n".join(full_text_list), "\n".join(log_lines), "\n".join(timestamp_text_list))

        except Exception as e:
            import traceback
            traceback.print_exc()
            raise Exception(f"Batch ASR Error: {str(e)}")
        finally:
            unload_asr_model()

